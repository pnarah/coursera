{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "500caac1",
   "metadata": {},
   "source": [
    "# Add Search Tool as Node in the graph\n",
    "\n",
    "https://app.tavily.com/home\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76976ed1",
   "metadata": {},
   "source": [
    "# 1. Install the search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fe8245",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U langchain-tavily --break-system-packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f9fb3d",
   "metadata": {},
   "source": [
    "# 2. Configure your environment\n",
    "Configure your environment with your search engine API key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e698b4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"....\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe82135",
   "metadata": {},
   "source": [
    "# 3. Define the tool¶\n",
    "Define the web search tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfd6cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "tool = TavilySearch(max_results=2)\n",
    "tools = [tool]\n",
    "tool.invoke(\"What's a 'node' in LangGraph?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e46056",
   "metadata": {},
   "source": [
    "# 4. Define the graph¶\n",
    "For the StateGraph you created in the first tutorial, add bind_tools on the LLM. This lets the LLM know the correct JSON format to use if it wants to use the search engine.\n",
    "\n",
    "Let's first select our LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89515b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U \"langchain[openai]\" --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083ea3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"....\"\n",
    "\n",
    "# llm = init_chat_model(\"openai:gpt-4.1\")\n",
    "llm = init_chat_model(\"openai:gpt-4.1-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39266f90",
   "metadata": {},
   "source": [
    "### We can now incorporate it into a StateGraph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68d27fa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x115f4d2b0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "# Modification: tell the LLM which tools it can call\n",
    "# highlight-next-line\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "def chatbot(state: State):\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fc8172",
   "metadata": {},
   "source": [
    "# 5. Create a function to run the tools\n",
    "Now, create a function to run the tools if they are called. Do this by adding the tools to a new node calledBasicToolNode that checks the most recent message in the state and calls tools if the message contains tool_calls. It relies on the LLM's tool_calling support, which is available in Anthropic, OpenAI, Google Gemini, and a number of other LLM providers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2eca286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tools available: {'tavily_search': TavilySearch(max_results=2, api_wrapper=TavilySearchAPIWrapper(tavily_api_key=SecretStr('**********')))}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x115f4d2b0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "\n",
    "class BasicToolNode:\n",
    "    \"\"\"A node that runs the tools requested in the last AIMessage.\"\"\"\n",
    "\n",
    "    def __init__(self, tools: list) -> None:\n",
    "        self.tools_by_name = {tool.name: tool for tool in tools}\n",
    "        print(\"Tools available:\", self.tools_by_name)\n",
    "\n",
    "    def __call__(self, inputs: dict):\n",
    "        if messages := inputs.get(\"messages\", []):\n",
    "            message = messages[-1]\n",
    "        else:\n",
    "            raise ValueError(\"No message found in input\")\n",
    "        outputs = []\n",
    "        for tool_call in message.tool_calls:\n",
    "            # print(\"Tool call:\", tool_call)\n",
    "            # if tool_call[\"name\"] not in self.tools_by_name:\n",
    "            #     raise ValueError(\n",
    "            #         f\"Tool '{tool_call['name']}' not found in available tools.\"\n",
    "            #     )\n",
    "            tool_result = self.tools_by_name[tool_call[\"name\"]].invoke(\n",
    "                tool_call[\"args\"]\n",
    "            )\n",
    "            outputs.append(\n",
    "                ToolMessage(\n",
    "                    content=json.dumps(tool_result),\n",
    "                    name=tool_call[\"name\"],\n",
    "                    tool_call_id=tool_call[\"id\"],\n",
    "                )\n",
    "            )\n",
    "        return {\"messages\": outputs}\n",
    "\n",
    "\n",
    "tool_node = BasicToolNode(tools=[tool])\n",
    "# print(\"Tool node created:\", tool_node)\n",
    "graph_builder.add_node(\"tools\", tool_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62c16a1",
   "metadata": {},
   "source": [
    "# 6. Define the conditional_edges¶\n",
    "With the tool node added, now you can define the conditional_edges.\n",
    "\n",
    "Edges route the control flow from one node to the next. Conditional edges start from a single node and usually contain \"if\" statements to route to different nodes depending on the current graph state. These functions receive the current graph state and return a string or list of strings indicating which node(s) to call next.\n",
    "\n",
    "Next, define a router function called route_tools that checks for tool_calls in the chatbot's output. Provide this function to the graph by calling add_conditional_edges, which tells the graph that whenever the chatbot node completes to check this function to see where to go next.\n",
    "\n",
    "The condition will route to tools if tool calls are present and END if not. Because the condition can return END, you do not need to explicitly set a finish_point this time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
