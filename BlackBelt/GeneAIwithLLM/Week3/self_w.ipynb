{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9b131c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade pip \n",
    "pip install tensorflow==2.18.0 keras=3.9.0\n",
    "pip install --no-dept torch==2.5.1 torchdata==0.0.0 --quiet \n",
    "pip install -U datasets==\n",
    "transfprmets \n",
    "evaluate\n",
    "rouge_score\n",
    "peft \n",
    "--quiet \n",
    "\n",
    "\n",
    "\n",
    "# TRL installation\n",
    "%pip install --no-deps git+https://github.com/lvwerra/trl.git@25fa1bd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac67d953",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"google/flan-t5-base\"\n",
    "huggingface_dataset_name=\"knkarthick/dialogsum\"\n",
    "dataset_original=load_dataset(huggingface_dataset_name)\n",
    "dataset_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317049ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(model_name,\n",
    "                  dataset_name,\n",
    "                  input_min_text_length,\n",
    "                  input_max_text_length):\n",
    "    dataset = load_dataset(dataset_name, split=\"train\")\n",
    "    dataset= dataset.filter(lambda x: len(x[\"dialogue\"]) >= input_min_text_length and len(x[\"dialogue\"]) <= input_max_text_length, batched=False)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, device_map=\"auto\")\n",
    "\n",
    "    def tokenize(sample):\n",
    "        prompt = f\"\"\"\n",
    "                Summarize the following conversation.\n",
    "\n",
    "                {sample[\"dialogue\"]}\n",
    "\n",
    "                Summary:\n",
    "                \"\"\"\n",
    "        sample[\"input_ids\"] = tokenizer.encode(prompt)\n",
    "        sample[\"query\"] = tokenize.decode(sample[\"input_ids\"])\n",
    "        return sample\n",
    "    dataset = dataset.map(tokenize, batched=False)\n",
    "    dataset.set_format(type=\"torch\")\n",
    "\n",
    "    dataset_split = dataset.train_test_split(test_size=0.2, shuffle=False seed=42)\n",
    "    return dataset_split\n",
    "    \n",
    "dataset= build_dataset(model_name=model_name,\n",
    "                       dataset_name=huggingface_dataset_name,\n",
    "                       input_min_text_length=200,\n",
    "                       input_max_text_length=1000)\n",
    "\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28a1529",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _,param  in model.named.parameters():\n",
    "        all_model_params += param.numel()       \n",
    "        if param.requires_grad:\n",
    "            trainable_parameters += param.numel()\n",
    "    print(f\"Number of trainable parameters: {trainable_parameters / 1e6:.2f}M\")\n",
    "    return f\"\\ntrainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
