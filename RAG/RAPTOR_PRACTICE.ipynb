{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0104428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /opt/homebrew/lib/python3.13/site-packages (0.3.25)\n",
      "Collecting langchain\n",
      "  Using cached langchain-0.3.26-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting umap-learn\n",
      "  Using cached umap_learn-0.5.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.7.0-cp313-cp313-macosx_12_0_arm64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: langchain_community in /opt/homebrew/lib/python3.13/site-packages (0.3.24)\n",
      "Collecting langchain_community\n",
      "  Using cached langchain_community-0.3.26-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: tiktoken in /opt/homebrew/lib/python3.13/site-packages (0.9.0)\n",
      "Requirement already satisfied: langchain-openai in /opt/homebrew/lib/python3.13/site-packages (0.3.24)\n",
      "Collecting langchain-openai\n",
      "  Using cached langchain_openai-0.3.25-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting langchainhub\n",
      "  Using cached langchainhub-0.1.21-py3-none-any.whl.metadata (659 bytes)\n",
      "Collecting langchain-chroma\n",
      "  Using cached langchain_chroma-0.2.4-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting langchain-anthropic\n",
      "  Using cached langchain_anthropic-0.3.15-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /opt/homebrew/lib/python3.13/site-packages (from langchain) (0.3.66)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /opt/homebrew/lib/python3.13/site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in /opt/homebrew/lib/python3.13/site-packages (from langchain) (0.4.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/homebrew/lib/python3.13/site-packages (from langchain) (2.11.5)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/homebrew/lib/python3.13/site-packages (from langchain) (1.4.23)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/homebrew/lib/python3.13/site-packages (from langchain) (2.32.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/homebrew/lib/python3.13/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/homebrew/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/homebrew/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/homebrew/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /opt/homebrew/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (4.13.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/homebrew/lib/python3.13/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/homebrew/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/homebrew/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/homebrew/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/homebrew/lib/python3.13/site-packages (from requests<3,>=2->langchain) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.13/site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.13/site-packages (from requests<3,>=2->langchain) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.13/site-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/lib/python3.13/site-packages (from umap-learn) (2.2.3)\n",
      "Collecting scipy>=1.3.1 (from umap-learn)\n",
      "  Using cached scipy-1.16.0-cp313-cp313-macosx_14_0_arm64.whl.metadata (61 kB)\n",
      "Collecting numba>=0.51.2 (from umap-learn)\n",
      "  Using cached numba-0.61.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (2.7 kB)\n",
      "Collecting pynndescent>=0.5 (from umap-learn)\n",
      "  Using cached pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/lib/python3.13/site-packages (from umap-learn) (4.67.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/homebrew/lib/python3.13/site-packages (from scikit-learn) (1.5.1)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/homebrew/lib/python3.13/site-packages (from langchain_community) (3.11.12)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/homebrew/lib/python3.13/site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /opt/homebrew/lib/python3.13/site-packages (from langchain_community) (2.9.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /opt/homebrew/lib/python3.13/site-packages (from langchain_community) (0.4.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/homebrew/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/homebrew/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/homebrew/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/homebrew/lib/python3.13/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/homebrew/lib/python3.13/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /opt/homebrew/lib/python3.13/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/homebrew/lib/python3.13/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.1.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/homebrew/lib/python3.13/site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.86.0 in /opt/homebrew/lib/python3.13/site-packages (from langchain-openai) (1.90.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/homebrew/lib/python3.13/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/homebrew/lib/python3.13/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/homebrew/lib/python3.13/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/homebrew/lib/python3.13/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: sniffio in /opt/homebrew/lib/python3.13/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/homebrew/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.86.0->langchain-openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/homebrew/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.86.0->langchain-openai) (0.16.0)\n",
      "Collecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub)\n",
      "  Using cached types_requests-2.32.4.20250611-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2->langchain)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting chromadb>=1.0.9 (from langchain-chroma)\n",
      "  Using cached chromadb-1.0.13-cp39-abi3-macosx_11_0_arm64.whl.metadata (7.0 kB)\n",
      "Collecting anthropic<1,>=0.52.0 (from langchain-anthropic)\n",
      "  Using cached anthropic-0.55.0-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting build>=1.0.3 (from chromadb>=1.0.9->langchain-chroma)\n",
      "  Using cached build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting pybase64>=1.4.1 (from chromadb>=1.0.9->langchain-chroma)\n",
      "  Using cached pybase64-1.4.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /opt/homebrew/lib/python3.13/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain-chroma) (0.34.2)\n",
      "Collecting posthog>=2.4.0 (from chromadb>=1.0.9->langchain-chroma)\n",
      "  Using cached posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb>=1.0.9->langchain-chroma)\n",
      "  Using cached onnxruntime-1.22.0-cp313-cp313-macosx_13_0_universal2.whl.metadata (4.5 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb>=1.0.9->langchain-chroma)\n",
      "  Using cached opentelemetry_api-1.34.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb>=1.0.9->langchain-chroma)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.34.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb>=1.0.9->langchain-chroma)\n",
      "  Using cached opentelemetry_sdk-1.34.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /opt/homebrew/lib/python3.13/site-packages (from chromadb>=1.0.9->langchain-chroma) (0.15.2)\n",
      "Collecting pypika>=0.48.9 (from chromadb>=1.0.9->langchain-chroma)\n",
      "  Using cached pypika-0.48.9-py2.py3-none-any.whl\n",
      "Collecting overrides>=7.3.1 (from chromadb>=1.0.9->langchain-chroma)\n",
      "  Using cached overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting importlib-resources (from chromadb>=1.0.9->langchain-chroma)\n",
      "  Using cached importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /opt/homebrew/lib/python3.13/site-packages (from chromadb>=1.0.9->langchain-chroma) (1.71.0)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb>=1.0.9->langchain-chroma)\n",
      "  Using cached bcrypt-4.3.0-cp39-abi3-macosx_10_12_universal2.whl.metadata (10 kB)\n",
      "Collecting typer>=0.9.0 (from chromadb>=1.0.9->langchain-chroma)\n",
      "  Using cached typer-0.16.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /opt/homebrew/lib/python3.13/site-packages (from chromadb>=1.0.9->langchain-chroma) (32.0.1)\n",
      "Collecting mmh3>=4.0.1 (from chromadb>=1.0.9->langchain-chroma)\n",
      "  Using cached mmh3-5.1.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (16 kB)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /opt/homebrew/lib/python3.13/site-packages (from chromadb>=1.0.9->langchain-chroma) (3.10.18)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/homebrew/lib/python3.13/site-packages (from chromadb>=1.0.9->langchain-chroma) (14.0.0)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in /opt/homebrew/lib/python3.13/site-packages (from chromadb>=1.0.9->langchain-chroma) (4.23.0)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb>=1.0.9->langchain-chroma)\n",
      "  Using cached pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/homebrew/lib/python3.13/site-packages (from jsonschema>=4.19.0->chromadb>=1.0.9->langchain-chroma) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/homebrew/lib/python3.13/site-packages (from jsonschema>=4.19.0->chromadb>=1.0.9->langchain-chroma) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/homebrew/lib/python3.13/site-packages (from jsonschema>=4.19.0->chromadb>=1.0.9->langchain-chroma) (0.22.3)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/pnarah/Library/Python/3.13/lib/python/site-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /Users/pnarah/Library/Python/3.13/lib/python/site-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /opt/homebrew/lib/python3.13/site-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (2.38.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/homebrew/lib/python3.13/site-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/homebrew/lib/python3.13/site-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /opt/homebrew/lib/python3.13/site-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (3.2.2)\n",
      "Requirement already satisfied: durationpy>=0.7 in /opt/homebrew/lib/python3.13/site-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (0.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/homebrew/lib/python3.13/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/homebrew/lib/python3.13/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/homebrew/lib/python3.13/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (4.9)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /opt/homebrew/lib/python3.13/site-packages (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (0.6.1)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/homebrew/lib/python3.13/site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /opt/homebrew/lib/python3.13/site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba>=0.51.2->umap-learn)\n",
      "  Using cached llvmlite-0.44.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (4.8 kB)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma)\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma)\n",
      "  Using cached flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Requirement already satisfied: protobuf in /opt/homebrew/lib/python3.13/site-packages (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma) (5.29.4)\n",
      "Requirement already satisfied: sympy in /opt/homebrew/lib/python3.13/site-packages (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma) (1.14.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /opt/homebrew/lib/python3.13/site-packages (from opentelemetry-api>=1.2.0->chromadb>=1.0.9->langchain-chroma) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /opt/homebrew/lib/python3.13/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb>=1.0.9->langchain-chroma) (3.21.0)\n",
      "Collecting googleapis-common-protos~=1.52 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.9->langchain-chroma)\n",
      "  Using cached googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.34.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.9->langchain-chroma)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_common-1.34.1-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting opentelemetry-proto==1.34.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.9->langchain-chroma)\n",
      "  Using cached opentelemetry_proto-1.34.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.55b1 (from opentelemetry-sdk>=1.2.0->chromadb>=1.0.9->langchain-chroma)\n",
      "  Using cached opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb>=1.0.9->langchain-chroma)\n",
      "  Using cached backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/homebrew/lib/python3.13/site-packages (from rich>=10.11.0->chromadb>=1.0.9->langchain-chroma) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/pnarah/Library/Python/3.13/lib/python/site-packages (from rich>=10.11.0->chromadb>=1.0.9->langchain-chroma) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/homebrew/lib/python3.13/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb>=1.0.9->langchain-chroma) (0.1.2)\n",
      "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /opt/homebrew/lib/python3.13/site-packages (from tokenizers>=0.13.2->chromadb>=1.0.9->langchain-chroma) (0.31.2)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.13/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=1.0.9->langchain-chroma) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/homebrew/lib/python3.13/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=1.0.9->langchain-chroma) (2023.10.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/homebrew/lib/python3.13/site-packages (from typer>=0.9.0->chromadb>=1.0.9->langchain-chroma) (8.1.8)\n",
      "Collecting shellingham>=1.3.0 (from typer>=0.9.0->chromadb>=1.0.9->langchain-chroma)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain-chroma)\n",
      "  Using cached httptools-0.6.4-cp313-cp313-macosx_11_0_arm64.whl.metadata (3.6 kB)\n",
      "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain-chroma)\n",
      "  Using cached uvloop-0.21.0-cp313-cp313-macosx_10_13_universal2.whl.metadata (4.9 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain-chroma)\n",
      "  Using cached watchfiles-1.1.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (4.9 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain-chroma)\n",
      "  Using cached websockets-15.0.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma)\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/lib/python3.13/site-packages (from sympy->onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma) (1.3.0)\n",
      "Using cached langchain-0.3.26-py3-none-any.whl (1.0 MB)\n",
      "Using cached umap_learn-0.5.7-py3-none-any.whl (88 kB)\n",
      "Using cached scikit_learn-1.7.0-cp313-cp313-macosx_12_0_arm64.whl (10.6 MB)\n",
      "Using cached langchain_community-0.3.26-py3-none-any.whl (2.5 MB)\n",
      "Using cached langchain_openai-0.3.25-py3-none-any.whl (69 kB)\n",
      "Using cached langchainhub-0.1.21-py3-none-any.whl (5.2 kB)\n",
      "Using cached types_requests-2.32.4.20250611-py3-none-any.whl (20 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Using cached langchain_chroma-0.2.4-py3-none-any.whl (11 kB)\n",
      "Using cached langchain_anthropic-0.3.15-py3-none-any.whl (28 kB)\n",
      "Using cached anthropic-0.55.0-py3-none-any.whl (289 kB)\n",
      "Using cached chromadb-1.0.13-cp39-abi3-macosx_11_0_arm64.whl (17.9 MB)\n",
      "Using cached bcrypt-4.3.0-cp39-abi3-macosx_10_12_universal2.whl (498 kB)\n",
      "Using cached build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
      "Using cached mmh3-5.1.0-cp313-cp313-macosx_11_0_arm64.whl (40 kB)\n",
      "Using cached numba-0.61.2-cp313-cp313-macosx_11_0_arm64.whl (2.8 MB)\n",
      "Using cached llvmlite-0.44.0-cp313-cp313-macosx_11_0_arm64.whl (26.2 MB)\n",
      "Using cached onnxruntime-1.22.0-cp313-cp313-macosx_13_0_universal2.whl (34.3 MB)\n",
      "Using cached opentelemetry_api-1.34.1-py3-none-any.whl (65 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_grpc-1.34.1-py3-none-any.whl (18 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_common-1.34.1-py3-none-any.whl (18 kB)\n",
      "Using cached opentelemetry_proto-1.34.1-py3-none-any.whl (55 kB)\n",
      "Using cached googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Using cached opentelemetry_sdk-1.34.1-py3-none-any.whl (118 kB)\n",
      "Using cached opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl (196 kB)\n",
      "Using cached overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Using cached posthog-5.4.0-py3-none-any.whl (105 kB)\n",
      "Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Using cached pybase64-1.4.1-cp313-cp313-macosx_11_0_arm64.whl (31 kB)\n",
      "Using cached pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
      "Using cached scipy-1.16.0-cp313-cp313-macosx_14_0_arm64.whl (20.7 MB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Using cached typer-0.16.0-py3-none-any.whl (46 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached httptools-0.6.4-cp313-cp313-macosx_11_0_arm64.whl (102 kB)\n",
      "Using cached uvloop-0.21.0-cp313-cp313-macosx_10_13_universal2.whl (1.5 MB)\n",
      "Using cached watchfiles-1.1.0-cp313-cp313-macosx_11_0_arm64.whl (393 kB)\n",
      "Using cached websockets-15.0.1-cp313-cp313-macosx_11_0_arm64.whl (173 kB)\n",
      "Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Using cached flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Using cached importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Using cached pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: pypika, flatbuffers, websockets, uvloop, urllib3, threadpoolctl, shellingham, scipy, pyproject_hooks, pybase64, overrides, opentelemetry-proto, mmh3, llvmlite, importlib-resources, humanfriendly, httptools, googleapis-common-protos, bcrypt, backoff, watchfiles, types-requests, scikit-learn, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, numba, coloredlogs, build, typer, pynndescent, posthog, opentelemetry-semantic-conventions, onnxruntime, langchainhub, anthropic, umap-learn, opentelemetry-sdk, opentelemetry-exporter-otlp-proto-grpc, langchain-openai, langchain-anthropic, chromadb, langchain-chroma, langchain, langchain_community\n",
      "\u001b[2K  Attempting uninstall: urllib3\n",
      "\u001b[2K    Found existing installation: urllib3 1.26.20\n",
      "\u001b[2K    Uninstalling urllib3-1.26.20:\n",
      "\u001b[2K      Successfully uninstalled urllib3-1.26.20\n",
      "\u001b[2K  Attempting uninstall: langchain-openai[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m35/44\u001b[0m [umap-learn]]ntic-conventions]\n",
      "\u001b[2K    Found existing installation: langchain-openai 0.3.24━━━━━━\u001b[0m \u001b[32m35/44\u001b[0m [umap-learn]\n",
      "\u001b[2K    Uninstalling langchain-openai-0.3.24:91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m35/44\u001b[0m [umap-learn]\n",
      "\u001b[2K      Successfully uninstalled langchain-openai-0.3.24━━━━━━━━\u001b[0m \u001b[32m35/44\u001b[0m [umap-learn]\n",
      "\u001b[2K  Attempting uninstall: langchain━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m41/44\u001b[0m [langchain-chroma]ic]\n",
      "\u001b[2K    Found existing installation: langchain 0.3.25m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m41/44\u001b[0m [langchain-chroma]\n",
      "\u001b[2K    Uninstalling langchain-0.3.25:━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m41/44\u001b[0m [langchain-chroma]\n",
      "\u001b[2K      Successfully uninstalled langchain-0.3.2590m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m41/44\u001b[0m [langchain-chroma]\n",
      "\u001b[2K  Attempting uninstall: langchain_community━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m42/44\u001b[0m [langchain]ma]\n",
      "\u001b[2K    Found existing installation: langchain-community 0.3.240m━\u001b[0m \u001b[32m42/44\u001b[0m [langchain]\n",
      "\u001b[2K    Uninstalling langchain-community-0.3.24:0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m42/44\u001b[0m [langchain]\n",
      "\u001b[2K      Successfully uninstalled langchain-community-0.3.24\u001b[0m \u001b[32m43/44\u001b[0m [langchain_community]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44/44\u001b[0m [langchain_community]ommunity]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "litellm 1.70.0 requires openai<1.76.0,>=1.68.2, but you have openai 1.90.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed anthropic-0.55.0 backoff-2.2.1 bcrypt-4.3.0 build-1.2.2.post1 chromadb-1.0.13 coloredlogs-15.0.1 flatbuffers-25.2.10 googleapis-common-protos-1.70.0 httptools-0.6.4 humanfriendly-10.0 importlib-resources-6.5.2 langchain-0.3.26 langchain-anthropic-0.3.15 langchain-chroma-0.2.4 langchain-openai-0.3.25 langchain_community-0.3.26 langchainhub-0.1.21 llvmlite-0.44.0 mmh3-5.1.0 numba-0.61.2 onnxruntime-1.22.0 opentelemetry-api-1.34.1 opentelemetry-exporter-otlp-proto-common-1.34.1 opentelemetry-exporter-otlp-proto-grpc-1.34.1 opentelemetry-proto-1.34.1 opentelemetry-sdk-1.34.1 opentelemetry-semantic-conventions-0.55b1 overrides-7.7.0 posthog-5.4.0 pybase64-1.4.1 pynndescent-0.5.13 pypika-0.48.9 pyproject_hooks-1.2.0 scikit-learn-1.7.0 scipy-1.16.0 shellingham-1.5.4 threadpoolctl-3.6.0 typer-0.16.0 types-requests-2.32.4.20250611 umap-learn-0.5.7 urllib3-2.5.0 uvloop-0.21.0 watchfiles-1.1.0 websockets-15.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --break-system-packages -U matplotlib langchain umap-learn scikit-learn langchain_community tiktoken langchain-openai langchainhub langchain-chroma langchain-anthropic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc9b3c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /opt/homebrew/lib/python3.13/site-packages (3.10.3)\n",
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.13.4-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/homebrew/lib/python3.13/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/homebrew/lib/python3.13/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/homebrew/lib/python3.13/site-packages (from matplotlib) (4.58.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/homebrew/lib/python3.13/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in /opt/homebrew/lib/python3.13/site-packages (from matplotlib) (2.2.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/lib/python3.13/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/homebrew/lib/python3.13/site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/homebrew/lib/python3.13/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/pnarah/Library/Python/3.13/lib/python/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4)\n",
      "  Downloading soupsieve-2.7-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /opt/homebrew/lib/python3.13/site-packages (from beautifulsoup4) (4.13.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/pnarah/Library/Python/3.13/lib/python/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Downloading beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
      "Downloading soupsieve-2.7-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [beautifulsoup4]\n",
      "\u001b[1A\u001b[2KSuccessfully installed beautifulsoup4-4.13.4 soupsieve-2.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --break-system-packages -U matplotlib beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7f78049",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tiktoken\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "\n",
    "# LCEL docs\n",
    "url = \"https://python.langchain.com/docs/expression_language/\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=20, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# LCEL w/ PydanticOutputParser (outside the primary LCEL docs)\n",
    "url = \"https://python.langchain.com/docs/how_to/output_parser_structured/\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=1, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs_pydantic = loader.load()\n",
    "\n",
    "# LCEL w/ Self Query (outside the primary LCEL docs)\n",
    "url = \"https://python.langchain.com/docs/how_to/self_query/\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=1, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs_sq = loader.load()\n",
    "\n",
    "# Doc texts\n",
    "docs.extend([*docs_pydantic, *docs_sq])\n",
    "docs_texts = [d.page_content for d in docs]\n",
    "\n",
    "# Calculate the number of tokens for each document\n",
    "counts = [num_tokens_from_string(d, \"cl100k_base\") for d in docs_texts]\n",
    "\n",
    "# Plotting the histogram of token counts\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(counts, bins=30, color=\"blue\", edgecolor=\"black\", alpha=0.7)\n",
    "plt.title(\"Histogram of Token Counts\")\n",
    "plt.xlabel(\"Token Count\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(axis=\"y\", alpha=0.75)\n",
    "\n",
    "# Display the histogram\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe1e00c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num tokens in all context: 10620\n"
     ]
    }
   ],
   "source": [
    "# Doc texts concat\n",
    "d_sorted = sorted(docs, key=lambda x: x.metadata[\"source\"])\n",
    "d_reversed = list(reversed(d_sorted))\n",
    "concatenated_content = \"\\n\\n\\n --- \\n\\n\\n\".join(\n",
    "    [doc.page_content for doc in d_reversed]\n",
    ")\n",
    "print(\n",
    "    \"Num tokens in all context: %s\"\n",
    "    % num_tokens_from_string(concatenated_content, \"cl100k_base\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca9f6222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\nHow to do \"self-querying\" retrieval | 🦜️🔗 LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentWe are growing and hiring for multiple roles for LangChain, LangGraph and LangSmith.  Join our team!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1💬SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem🦜🛠️ LangSmith🦜🕸️ LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyHow-to guidesHow to do \"self-querying\" retrievalOn this pageHow to do \"self-querying\" retrieval\\ninfoHead to Integrations for documentation on vector stores with built-in support for self-querying.\\nA self-querying retriever is one that, as the name suggests, has the ability to query itself. Specifically, given any natural language query, the retriever uses a query-constructing LLM chain to write a structured query and then applies that structured query to its underlying vector store. This allows the retriever to not only use the user-input query for semantic similarity comparison with the contents of stored documents but to also extract filters from the user query on the metadata of stored documents and to execute those filters.\\n\\nGet started\\u200b\\nFor demonstration purposes we\\'ll use a Chroma vector store. We\\'ve created a small demo set of documents that contain summaries of movies.\\nNote: The self-query retriever requires you to have lark package installed.\\n%pip install --upgrade --quiet  lark langchain-chroma\\nfrom langchain_chroma import Chromafrom langchain_core.documents import Documentfrom langchain_openai import OpenAIEmbeddingsdocs = [    Document(        page_content=\"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",        metadata={\"year\": 1993, \"rating\": 7.7, \"genre\": \"science fiction\"},    ),    Document(        page_content=\"Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\",        metadata={\"year\": 2010, \"director\": \"Christopher Nolan\", \"rating\": 8.2},    ),    Document(        page_content=\"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\",        metadata={\"year\": 2006, \"director\": \"Satoshi Kon\", \"rating\": 8.6},    ),    Document(        page_content=\"A bunch of normal-sized women are supremely wholesome and some men pine after them\",        metadata={\"year\": 2019, \"director\": \"Greta Gerwig\", \"rating\": 8.3},    ),    Document(        page_content=\"Toys come alive and have a blast doing so\",        metadata={\"year\": 1995, \"genre\": \"animated\"},    ),    Document(        page_content=\"Three men walk into the Zone, three men walk out of the Zone\",        metadata={            \"year\": 1979,            \"director\": \"Andrei Tarkovsky\",            \"genre\": \"thriller\",            \"rating\": 9.9,        },    ),]vectorstore = Chroma.from_documents(docs, OpenAIEmbeddings())API Reference:Document | OpenAIEmbeddings\\nCreating our self-querying retriever\\u200b\\nNow we can instantiate our retriever. To do this we\\'ll need to provide some information upfront about the metadata fields that our documents support and a short description of the document contents.\\nfrom langchain.chains.query_constructor.schema import AttributeInfofrom langchain.retrievers.self_query.base import SelfQueryRetrieverfrom langchain_openai import ChatOpenAImetadata_field_info = [    AttributeInfo(        name=\"genre\",        description=\"The genre of the movie. One of [\\'science fiction\\', \\'comedy\\', \\'drama\\', \\'thriller\\', \\'romance\\', \\'action\\', \\'animated\\']\",        type=\"string\",    ),    AttributeInfo(        name=\"year\",        description=\"The year the movie was released\",        type=\"integer\",    ),    AttributeInfo(        name=\"director\",        description=\"The name of the movie director\",        type=\"string\",    ),    AttributeInfo(        name=\"rating\", description=\"A 1-10 rating for the movie\", type=\"float\"    ),]document_content_description = \"Brief summary of a movie\"llm = ChatOpenAI(temperature=0)retriever = SelfQueryRetriever.from_llm(    llm,    vectorstore,    document_content_description,    metadata_field_info,)API Reference:AttributeInfo | SelfQueryRetriever | ChatOpenAI\\nTesting it out\\u200b\\nAnd now we can actually try using our retriever!\\n# This example only specifies a filterretriever.invoke(\"I want to watch a movie rated higher than 8.5\")\\n[Document(page_content=\\'Three men walk into the Zone, three men walk out of the Zone\\', metadata={\\'director\\': \\'Andrei Tarkovsky\\', \\'genre\\': \\'thriller\\', \\'rating\\': 9.9, \\'year\\': 1979}), Document(page_content=\\'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\\', metadata={\\'director\\': \\'Satoshi Kon\\', \\'rating\\': 8.6, \\'year\\': 2006})]\\n# This example specifies a query and a filterretriever.invoke(\"Has Greta Gerwig directed any movies about women\")\\n[Document(page_content=\\'A bunch of normal-sized women are supremely wholesome and some men pine after them\\', metadata={\\'director\\': \\'Greta Gerwig\\', \\'rating\\': 8.3, \\'year\\': 2019})]\\n# This example specifies a composite filterretriever.invoke(\"What\\'s a highly rated (above 8.5) science fiction film?\")\\n[Document(page_content=\\'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\\', metadata={\\'director\\': \\'Satoshi Kon\\', \\'rating\\': 8.6, \\'year\\': 2006}), Document(page_content=\\'Three men walk into the Zone, three men walk out of the Zone\\', metadata={\\'director\\': \\'Andrei Tarkovsky\\', \\'genre\\': \\'thriller\\', \\'rating\\': 9.9, \\'year\\': 1979})]\\n# This example specifies a query and composite filterretriever.invoke(    \"What\\'s a movie after 1990 but before 2005 that\\'s all about toys, and preferably is animated\")\\n[Document(page_content=\\'Toys come alive and have a blast doing so\\', metadata={\\'genre\\': \\'animated\\', \\'year\\': 1995})]\\nFilter k\\u200b\\nWe can also use the self query retriever to specify k: the number of documents to fetch.\\nWe can do this by passing enable_limit=True to the constructor.\\nretriever = SelfQueryRetriever.from_llm(    llm,    vectorstore,    document_content_description,    metadata_field_info,    enable_limit=True,)# This example only specifies a relevant queryretriever.invoke(\"What are two movies about dinosaurs\")\\n[Document(page_content=\\'A bunch of scientists bring back dinosaurs and mayhem breaks loose\\', metadata={\\'genre\\': \\'science fiction\\', \\'rating\\': 7.7, \\'year\\': 1993}), Document(page_content=\\'Toys come alive and have a blast doing so\\', metadata={\\'genre\\': \\'animated\\', \\'year\\': 1995})]\\nConstructing from scratch with LCEL\\u200b\\nTo see what\\'s going on under the hood, and to have more custom control, we can reconstruct our retriever from scratch.\\nFirst, we need to create a query-construction chain. This chain will take a user query and generated a StructuredQuery object which captures the filters specified by the user. We provide some helper functions for creating a prompt and output parser. These have a number of tunable params that we\\'ll ignore here for simplicity.\\nfrom langchain.chains.query_constructor.base import (    StructuredQueryOutputParser,    get_query_constructor_prompt,)prompt = get_query_constructor_prompt(    document_content_description,    metadata_field_info,)output_parser = StructuredQueryOutputParser.from_components()query_constructor = prompt | llm | output_parserAPI Reference:StructuredQueryOutputParser | get_query_constructor_prompt\\nLet\\'s look at our prompt:\\nprint(prompt.format(query=\"dummy question\"))\\nYour goal is to structure the user\\'s query to match the request schema provided below.<< Structured Request Schema >>When responding use a markdown code snippet with a JSON object formatted in the following schema:\\\\`\\\\`\\\\`json{    \"query\": string \\\\ text string to compare to document contents    \"filter\": string \\\\ logical condition statement for filtering documents}\\\\`\\\\`\\\\`The query string should contain only text that is expected to match the contents of documents. Any conditions in the filter should not be mentioned in the query as well.A logical condition statement is composed of one or more comparison and logical operation statements.A comparison statement takes the form: `comp(attr, val)`:- `comp` (eq | ne | gt | gte | lt | lte | contain | like | in | nin): comparator- `attr` (string):  name of attribute to apply the comparison to- `val` (string): is the comparison valueA logical operation statement takes the form `op(statement1, statement2, ...)`:- `op` (and | or | not): logical operator- `statement1`, `statement2`, ... (comparison statements or logical operation statements): one or more statements to apply the operation toMake sure that you only use the comparators and logical operators listed above and no others.Make sure that filters only refer to attributes that exist in the data source.Make sure that filters only use the attributed names with its function names if there are functions applied on them.Make sure that filters only use format `YYYY-MM-DD` when handling date data typed values.Make sure that filters take into account the descriptions of attributes and only make comparisons that are feasible given the type of data being stored.Make sure that filters are only used as needed. If there are no filters that should be applied return \"NO_FILTER\" for the filter value.<< Example 1. >>Data Source:\\\\`\\\\`\\\\`json{    \"content\": \"Lyrics of a song\",    \"attributes\": {        \"artist\": {            \"type\": \"string\",            \"description\": \"Name of the song artist\"        },        \"length\": {            \"type\": \"integer\",            \"description\": \"Length of the song in seconds\"        },        \"genre\": {            \"type\": \"string\",            \"description\": \"The song genre, one of \"pop\", \"rock\" or \"rap\"\"        }    }}\\\\`\\\\`\\\\`User Query:What are songs by Taylor Swift or Katy Perry about teenage romance under 3 minutes long in the dance pop genreStructured Request:\\\\`\\\\`\\\\`json{    \"query\": \"teenager love\",    \"filter\": \"and(or(eq(\\\\\"artist\\\\\", \\\\\"Taylor Swift\\\\\"), eq(\\\\\"artist\\\\\", \\\\\"Katy Perry\\\\\")), lt(\\\\\"length\\\\\", 180), eq(\\\\\"genre\\\\\", \\\\\"pop\\\\\"))\"}\\\\`\\\\`\\\\`<< Example 2. >>Data Source:\\\\`\\\\`\\\\`json{    \"content\": \"Lyrics of a song\",    \"attributes\": {        \"artist\": {            \"type\": \"string\",            \"description\": \"Name of the song artist\"        },        \"length\": {            \"type\": \"integer\",            \"description\": \"Length of the song in seconds\"        },        \"genre\": {            \"type\": \"string\",            \"description\": \"The song genre, one of \"pop\", \"rock\" or \"rap\"\"        }    }}\\\\`\\\\`\\\\`User Query:What are songs that were not published on SpotifyStructured Request:\\\\`\\\\`\\\\`json{    \"query\": \"\",    \"filter\": \"NO_FILTER\"}\\\\`\\\\`\\\\`<< Example 3. >>Data Source:\\\\`\\\\`\\\\`json{    \"content\": \"Brief summary of a movie\",    \"attributes\": {    \"genre\": {        \"description\": \"The genre of the movie. One of [\\'science fiction\\', \\'comedy\\', \\'drama\\', \\'thriller\\', \\'romance\\', \\'action\\', \\'animated\\']\",        \"type\": \"string\"    },    \"year\": {        \"description\": \"The year the movie was released\",        \"type\": \"integer\"    },    \"director\": {        \"description\": \"The name of the movie director\",        \"type\": \"string\"    },    \"rating\": {        \"description\": \"A 1-10 rating for the movie\",        \"type\": \"float\"    }}}\\\\`\\\\`\\\\`User Query:dummy questionStructured Request:\\nAnd what our full chain produces:\\nquery_constructor.invoke(    {        \"query\": \"What are some sci-fi movies from the 90\\'s directed by Luc Besson about taxi drivers\"    })\\nStructuredQuery(query=\\'taxi driver\\', filter=Operation(operator=<Operator.AND: \\'and\\'>, arguments=[Comparison(comparator=<Comparator.EQ: \\'eq\\'>, attribute=\\'genre\\', value=\\'science fiction\\'), Operation(operator=<Operator.AND: \\'and\\'>, arguments=[Comparison(comparator=<Comparator.GTE: \\'gte\\'>, attribute=\\'year\\', value=1990), Comparison(comparator=<Comparator.LT: \\'lt\\'>, attribute=\\'year\\', value=2000)]), Comparison(comparator=<Comparator.EQ: \\'eq\\'>, attribute=\\'director\\', value=\\'Luc Besson\\')]), limit=None)\\nThe query constructor is the key element of the self-query retriever. To make a great retrieval system you\\'ll need to make sure your query constructor works well. Often this requires adjusting the prompt, the examples in the prompt, the attribute descriptions, etc. For an example that walks through refining a query constructor on some hotel inventory data, check out this cookbook.\\nThe next key element is the structured query translator. This is the object responsible for translating the generic StructuredQuery object into a metadata filter in the syntax of the vector store you\\'re using. LangChain comes with a number of built-in translators. To see them all head to the Integrations section.\\nfrom langchain_community.query_constructors.chroma import ChromaTranslatorretriever = SelfQueryRetriever(    query_constructor=query_constructor,    vectorstore=vectorstore,    structured_query_translator=ChromaTranslator(),)API Reference:ChromaTranslator\\nretriever.invoke(    \"What\\'s a movie after 1990 but before 2005 that\\'s all about toys, and preferably is animated\")\\n[Document(page_content=\\'Toys come alive and have a blast doing so\\', metadata={\\'genre\\': \\'animated\\', \\'year\\': 1995})]Edit this pageWas this page helpful?PreviousHow to pass runtime secrets to runnablesNextHow to split text based on semantic similarityGet startedCreating our self-querying retrieverTesting it outFilter kConstructing from scratch with LCELCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright © 2025 LangChain, Inc.\\n\\n\\n\\n\\n --- \\n\\n\\n\\n\\n\\n\\n\\nHow to use output parsers to parse an LLM response into structured format | 🦜️🔗 LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentWe are growing and hiring for multiple roles for LangChain, LangGraph and LangSmith.  Join our team!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1💬SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem🦜🛠️ LangSmith🦜🕸️ LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyHow-to guidesHow to use output parsers to parse an LLM response into structured formatOn this pageHow to use output parsers to parse an LLM response into structured format\\nLanguage models output text. But there are times where you want to get more structured information than just text back. While some model providers support built-in ways to return structured output, not all do.\\nOutput parsers are classes that help structure language model responses. There are two main methods an output parser must implement:\\n\\n\"Get format instructions\": A method which returns a string containing instructions for how the output of a language model should be formatted.\\n\"Parse\": A method which takes in a string (assumed to be the response from a language model) and parses it into some structure.\\n\\nAnd then one optional one:\\n\\n\"Parse with prompt\": A method which takes in a string (assumed to be the response from a language model) and a prompt (assumed to be the prompt that generated such a response) and parses it into some structure. The prompt is largely provided in the event the OutputParser wants to retry or fix the output in some way, and needs information from the prompt to do so.\\n\\nGet started\\u200b\\nBelow we go over the main type of output parser, the PydanticOutputParser.\\nfrom langchain_core.output_parsers import PydanticOutputParserfrom langchain_core.prompts import PromptTemplatefrom langchain_openai import OpenAIfrom pydantic import BaseModel, Field, model_validatormodel = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0.0)# Define your desired data structure.class Joke(BaseModel):    setup: str = Field(description=\"question to set up a joke\")    punchline: str = Field(description=\"answer to resolve the joke\")    # You can add custom validation logic easily with Pydantic.    @model_validator(mode=\"before\")    @classmethod    def question_ends_with_question_mark(cls, values: dict) -> dict:        setup = values.get(\"setup\")        if setup and setup[-1] != \"?\":            raise ValueError(\"Badly formed question!\")        return values# Set up a parser + inject instructions into the prompt template.parser = PydanticOutputParser(pydantic_object=Joke)prompt = PromptTemplate(    template=\"Answer the user query.\\\\n{format_instructions}\\\\n{query}\\\\n\",    input_variables=[\"query\"],    partial_variables={\"format_instructions\": parser.get_format_instructions()},)# And a query intended to prompt a language model to populate the data structure.prompt_and_model = prompt | modeloutput = prompt_and_model.invoke({\"query\": \"Tell me a joke.\"})parser.invoke(output)API Reference:PydanticOutputParser | PromptTemplate | OpenAI\\nJoke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw the salad dressing!\\')\\nLCEL\\u200b\\nOutput parsers implement the Runnable interface, the basic building block of the LangChain Expression Language (LCEL). This means they support invoke, ainvoke, stream, astream, batch, abatch, astream_log calls.\\nOutput parsers accept a string or BaseMessage as input and can return an arbitrary type.\\nparser.invoke(output)\\nJoke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw the salad dressing!\\')\\nInstead of manually invoking the parser, we also could\\'ve just added it to our Runnable sequence:\\nchain = prompt | model | parserchain.invoke({\"query\": \"Tell me a joke.\"})\\nJoke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw the salad dressing!\\')\\nWhile all parsers support the streaming interface, only certain parsers can stream through partially parsed objects, since this is highly dependent on the output type. Parsers which cannot construct partial objects will simply yield the fully parsed output.\\nThe SimpleJsonOutputParser for example can stream through partial outputs:\\nfrom langchain.output_parsers.json import SimpleJsonOutputParserjson_prompt = PromptTemplate.from_template(    \"Return a JSON object with an `answer` key that answers the following question: {question}\")json_parser = SimpleJsonOutputParser()json_chain = json_prompt | model | json_parserAPI Reference:SimpleJsonOutputParser\\nlist(json_chain.stream({\"question\": \"Who invented the microscope?\"}))\\n[{}, {\\'answer\\': \\'\\'}, {\\'answer\\': \\'Ant\\'}, {\\'answer\\': \\'Anton\\'}, {\\'answer\\': \\'Antonie\\'}, {\\'answer\\': \\'Antonie van\\'}, {\\'answer\\': \\'Antonie van Lee\\'}, {\\'answer\\': \\'Antonie van Leeu\\'}, {\\'answer\\': \\'Antonie van Leeuwen\\'}, {\\'answer\\': \\'Antonie van Leeuwenho\\'}, {\\'answer\\': \\'Antonie van Leeuwenhoek\\'}]\\nSimilarly,for PydanticOutputParser:\\nlist(chain.stream({\"query\": \"Tell me a joke.\"}))\\n[Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'\\'), Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because\\'), Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it\\'), Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw\\'), Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw the\\'), Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw the salad\\'), Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw the salad dressing\\'), Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw the salad dressing!\\')]Edit this pageWas this page helpful?PreviousHow to run custom functionsNextHow to handle cases where no queries are generatedGet startedLCELCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright © 2025 LangChain, Inc.\\n\\n\\n\\n\\n --- \\n\\n\\n\\n\\n\\n\\n\\nLangChain Expression Language (LCEL) | 🦜️🔗 LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentWe are growing and hiring for multiple roles for LangChain, LangGraph and LangSmith.  Join our team!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1💬SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem🦜🛠️ LangSmith🦜🕸️ LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideLangChain Expression Language (LCEL)On this pageLangChain Expression Language (LCEL)\\nPrerequisites\\nRunnable Interface\\n\\nThe LangChain Expression Language (LCEL) takes a declarative approach to building new Runnables from existing Runnables.\\nThis means that you describe what should happen, rather than how it should happen, allowing LangChain to optimize the run-time execution of the chains.\\nWe often refer to a Runnable created using LCEL as a \"chain\". It\\'s important to remember that a \"chain\" is Runnable and it implements the full Runnable Interface.\\nnote\\nThe LCEL cheatsheet shows common patterns that involve the Runnable interface and LCEL expressions.\\nPlease see the following list of how-to guides that cover common tasks with LCEL.\\nA list of built-in Runnables can be found in the LangChain Core API Reference. Many of these Runnables are useful when composing custom \"chains\" in LangChain using LCEL.\\n\\nBenefits of LCEL\\u200b\\nLangChain optimizes the run-time execution of chains built with LCEL in a number of ways:\\n\\nOptimized parallel execution: Run Runnables in parallel using RunnableParallel or run multiple inputs through a given chain in parallel using the Runnable Batch API. Parallel execution can significantly reduce the latency as processing can be done in parallel instead of sequentially.\\nGuaranteed Async support: Any chain built with LCEL can be run asynchronously using the Runnable Async API. This can be useful when running chains in a server environment where you want to handle large number of requests concurrently.\\nSimplify streaming: LCEL chains can be streamed, allowing for incremental output as the chain is executed. LangChain can optimize the streaming of the output to minimize the time-to-first-token(time elapsed until the first chunk of output from a chat model or llm comes out).\\n\\nOther benefits include:\\n\\nSeamless LangSmith tracing\\nAs your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step.\\nWith LCEL, all steps are automatically logged to LangSmith for maximum observability and debuggability.\\nStandard API: Because all chains are built using the Runnable interface, they can be used in the same way as any other Runnable.\\nDeployable with LangServe: Chains built with LCEL can be deployed using for production use.\\n\\nShould I use LCEL?\\u200b\\nLCEL is an orchestration solution -- it allows LangChain to handle run-time execution of chains in an optimized way.\\nWhile we have seen users run chains with hundreds of steps in production, we generally recommend using LCEL for simpler orchestration tasks. When the application requires complex state management, branching, cycles or multiple agents, we recommend that users take advantage of LangGraph.\\nIn LangGraph, users define graphs that specify the application\\'s flow. This allows users to keep using LCEL within individual nodes when LCEL is needed, while making it easy to define complex orchestration logic that is more readable and maintainable.\\nHere are some guidelines:\\n\\nIf you are making a single LLM call, you don\\'t need LCEL; instead call the underlying chat model directly.\\nIf you have a simple chain (e.g., prompt + llm + parser, simple retrieval set up etc.), LCEL is a reasonable fit, if you\\'re taking advantage of the LCEL benefits.\\nIf you\\'re building a complex chain (e.g., with branching, cycles, multiple agents, etc.) use LangGraph instead. Remember that you can always use LCEL within individual nodes in LangGraph.\\n\\nComposition Primitives\\u200b\\nLCEL chains are built by composing existing Runnables together. The two main composition primitives are RunnableSequence and RunnableParallel.\\nMany other composition primitives (e.g., RunnableAssign) can be thought of as variations of these two primitives.\\nnoteYou can find a list of all composition primitives in the LangChain Core API Reference.\\nRunnableSequence\\u200b\\nRunnableSequence is a composition primitive that allows you \"chain\" multiple runnables sequentially, with the output of one runnable serving as the input to the next.\\nfrom langchain_core.runnables import RunnableSequencechain = RunnableSequence([runnable1, runnable2])API Reference:RunnableSequence\\nInvoking the chain with some input:\\nfinal_output = chain.invoke(some_input)\\ncorresponds to the following:\\noutput1 = runnable1.invoke(some_input)final_output = runnable2.invoke(output1)\\nnoterunnable1 and runnable2 are placeholders for any Runnable that you want to chain together.\\nRunnableParallel\\u200b\\nRunnableParallel is a composition primitive that allows you to run multiple runnables concurrently, with the same input provided to each.\\nfrom langchain_core.runnables import RunnableParallelchain = RunnableParallel({    \"key1\": runnable1,    \"key2\": runnable2,})API Reference:RunnableParallel\\nInvoking the chain with some input:\\nfinal_output = chain.invoke(some_input)\\nWill yield a final_output dictionary with the same keys as the input dictionary, but with the values replaced by the output of the corresponding runnable.\\n{    \"key1\": runnable1.invoke(some_input),    \"key2\": runnable2.invoke(some_input),}\\nRecall, that the runnables are executed in parallel, so while the result is the same as\\ndictionary comprehension shown above, the execution time is much faster.\\nnoteRunnableParallelsupports both synchronous and asynchronous execution (as all Runnables do).\\nFor synchronous execution, RunnableParallel uses a ThreadPoolExecutor to run the runnables concurrently.\\nFor asynchronous execution, RunnableParallel uses asyncio.gather to run the runnables concurrently.\\n\\nComposition Syntax\\u200b\\nThe usage of RunnableSequence and RunnableParallel is so common that we created a shorthand syntax for using them. This helps\\nto make the code more readable and concise.\\nThe | operator\\u200b\\nWe have overloaded the | operator to create a RunnableSequence from two Runnables.\\nchain = runnable1 | runnable2\\nis Equivalent to:\\nchain = RunnableSequence([runnable1, runnable2])\\nThe .pipe method\\u200b\\nIf you have moral qualms with operator overloading, you can use the .pipe method instead. This is equivalent to the | operator.\\nchain = runnable1.pipe(runnable2)\\nCoercion\\u200b\\nLCEL applies automatic type coercion to make it easier to compose chains.\\nIf you do not understand the type coercion, you can always use the RunnableSequence and RunnableParallel classes directly.\\nThis will make the code more verbose, but it will also make it more explicit.\\nDictionary to RunnableParallel\\u200b\\nInside an LCEL expression, a dictionary is automatically converted to a RunnableParallel.\\nFor example, the following code:\\nmapping = {    \"key1\": runnable1,    \"key2\": runnable2,}chain = mapping | runnable3\\nIt gets automatically converted to the following:\\nchain = RunnableSequence([RunnableParallel(mapping), runnable3])\\ncautionYou have to be careful because the mapping dictionary is not a RunnableParallel object, it is just a dictionary. This means that the following code will raise an AttributeError:mapping.invoke(some_input)\\nFunction to RunnableLambda\\u200b\\nInside an LCEL expression, a function is automatically converted to a RunnableLambda.\\ndef some_func(x):    return xchain = some_func | runnable1\\nIt gets automatically converted to the following:\\nchain = RunnableSequence([RunnableLambda(some_func), runnable1])\\ncautionYou have to be careful because the lambda function is not a RunnableLambda object, it is just a function. This means that the following code will raise an AttributeError:lambda x: x + 1.invoke(some_input)\\nLegacy chains\\u200b\\nLCEL aims to provide consistency around behavior and customization over legacy subclassed chains such as LLMChain and\\nConversationalRetrievalChain. Many of these legacy chains hide important details like prompts, and as a wider variety\\nof viable models emerge, customization has become more and more important.\\nIf you are currently using one of these legacy chains, please see this guide for guidance on how to migrate.\\nFor guides on how to do specific tasks with LCEL, check out the relevant how-to guides.Edit this pageWas this page helpful?PreviousKey-value storesNextMessagesBenefits of LCELShould I use LCEL?Composition PrimitivesRunnableSequenceRunnableParallelComposition SyntaxThe | operatorThe .pipe methodCoercionLegacy chainsCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright © 2025 LangChain, Inc.\\n\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenated_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "268d7b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doc texts split\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "chunk_size_tok = 2000\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=chunk_size_tok, chunk_overlap=0\n",
    ")\n",
    "texts_split = text_splitter.split_text(concatenated_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7365bfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2fe498",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "We can test various models, including the new Claude3 family.\n",
    "\n",
    "Be sure to set the relevant API keys:\n",
    "\n",
    "ANTHROPIC_API_KEY\n",
    "OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4ced5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embd = OpenAIEmbeddings()\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "model = ChatOpenAI(temperature=0, model=\"gpt-4-1106-preview\")\n",
    "\n",
    "# from langchain_anthropic import ChatAnthropic\n",
    "# model = ChatAnthropic(temperature=0, model=\"claude-3-opus-20240229\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa55d106",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import umap\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "RANDOM_SEED = 224  # Fixed seed for reproducibility\n",
    "\n",
    "### --- Code from citations referenced above (added comments and docstrings) --- ###\n",
    "\n",
    "\n",
    "def global_cluster_embeddings(\n",
    "    embeddings: np.ndarray,\n",
    "    dim: int,\n",
    "    n_neighbors: Optional[int] = None,\n",
    "    metric: str = \"cosine\",\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Perform global dimensionality reduction on the embeddings using UMAP.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - dim: The target dimensionality for the reduced space.\n",
    "    - n_neighbors: Optional; the number of neighbors to consider for each point.\n",
    "                   If not provided, it defaults to the square root of the number of embeddings.\n",
    "    - metric: The distance metric to use for UMAP.\n",
    "\n",
    "    Returns:\n",
    "    - A numpy array of the embeddings reduced to the specified dimensionality.\n",
    "    \"\"\"\n",
    "    if n_neighbors is None:\n",
    "        n_neighbors = int((len(embeddings) - 1) ** 0.5)\n",
    "    return umap.UMAP(\n",
    "        n_neighbors=n_neighbors, n_components=dim, metric=metric\n",
    "    ).fit_transform(embeddings)\n",
    "\n",
    "\n",
    "def local_cluster_embeddings(\n",
    "    embeddings: np.ndarray, dim: int, num_neighbors: int = 10, metric: str = \"cosine\"\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Perform local dimensionality reduction on the embeddings using UMAP, typically after global clustering.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - dim: The target dimensionality for the reduced space.\n",
    "    - num_neighbors: The number of neighbors to consider for each point.\n",
    "    - metric: The distance metric to use for UMAP.\n",
    "\n",
    "    Returns:\n",
    "    - A numpy array of the embeddings reduced to the specified dimensionality.\n",
    "    \"\"\"\n",
    "    return umap.UMAP(\n",
    "        n_neighbors=num_neighbors, n_components=dim, metric=metric\n",
    "    ).fit_transform(embeddings)\n",
    "\n",
    "\n",
    "def get_optimal_clusters(\n",
    "    embeddings: np.ndarray, max_clusters: int = 50, random_state: int = RANDOM_SEED\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Determine the optimal number of clusters using the Bayesian Information Criterion (BIC) with a Gaussian Mixture Model.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - max_clusters: The maximum number of clusters to consider.\n",
    "    - random_state: Seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - An integer representing the optimal number of clusters found.\n",
    "    \"\"\"\n",
    "    max_clusters = min(max_clusters, len(embeddings))\n",
    "    n_clusters = np.arange(1, max_clusters)\n",
    "    bics = []\n",
    "    for n in n_clusters:\n",
    "        gm = GaussianMixture(n_components=n, random_state=random_state)\n",
    "        gm.fit(embeddings)\n",
    "        bics.append(gm.bic(embeddings))\n",
    "    return n_clusters[np.argmin(bics)]\n",
    "\n",
    "\n",
    "def GMM_cluster(embeddings: np.ndarray, threshold: float, random_state: int = 0):\n",
    "    \"\"\"\n",
    "    Cluster embeddings using a Gaussian Mixture Model (GMM) based on a probability threshold.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - threshold: The probability threshold for assigning an embedding to a cluster.\n",
    "    - random_state: Seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - A tuple containing the cluster labels and the number of clusters determined.\n",
    "    \"\"\"\n",
    "    n_clusters = get_optimal_clusters(embeddings)\n",
    "    gm = GaussianMixture(n_components=n_clusters, random_state=random_state)\n",
    "    gm.fit(embeddings)\n",
    "    probs = gm.predict_proba(embeddings)\n",
    "    labels = [np.where(prob > threshold)[0] for prob in probs]\n",
    "    return labels, n_clusters\n",
    "\n",
    "\n",
    "def perform_clustering(\n",
    "    embeddings: np.ndarray,\n",
    "    dim: int,\n",
    "    threshold: float,\n",
    ") -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Perform clustering on the embeddings by first reducing their dimensionality globally, then clustering\n",
    "    using a Gaussian Mixture Model, and finally performing local clustering within each global cluster.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - dim: The target dimensionality for UMAP reduction.\n",
    "    - threshold: The probability threshold for assigning an embedding to a cluster in GMM.\n",
    "\n",
    "    Returns:\n",
    "    - A list of numpy arrays, where each array contains the cluster IDs for each embedding.\n",
    "    \"\"\"\n",
    "    if len(embeddings) <= dim + 1:\n",
    "        # Avoid clustering when there's insufficient data\n",
    "        return [np.array([0]) for _ in range(len(embeddings))]\n",
    "\n",
    "    # Global dimensionality reduction\n",
    "    reduced_embeddings_global = global_cluster_embeddings(embeddings, dim)\n",
    "    # Global clustering\n",
    "    global_clusters, n_global_clusters = GMM_cluster(\n",
    "        reduced_embeddings_global, threshold\n",
    "    )\n",
    "\n",
    "    all_local_clusters = [np.array([]) for _ in range(len(embeddings))]\n",
    "    total_clusters = 0\n",
    "\n",
    "    # Iterate through each global cluster to perform local clustering\n",
    "    for i in range(n_global_clusters):\n",
    "        # Extract embeddings belonging to the current global cluster\n",
    "        global_cluster_embeddings_ = embeddings[\n",
    "            np.array([i in gc for gc in global_clusters])\n",
    "        ]\n",
    "\n",
    "        if len(global_cluster_embeddings_) == 0:\n",
    "            continue\n",
    "        if len(global_cluster_embeddings_) <= dim + 1:\n",
    "            # Handle small clusters with direct assignment\n",
    "            local_clusters = [np.array([0]) for _ in global_cluster_embeddings_]\n",
    "            n_local_clusters = 1\n",
    "        else:\n",
    "            # Local dimensionality reduction and clustering\n",
    "            reduced_embeddings_local = local_cluster_embeddings(\n",
    "                global_cluster_embeddings_, dim\n",
    "            )\n",
    "            local_clusters, n_local_clusters = GMM_cluster(\n",
    "                reduced_embeddings_local, threshold\n",
    "            )\n",
    "\n",
    "        # Assign local cluster IDs, adjusting for total clusters already processed\n",
    "        for j in range(n_local_clusters):\n",
    "            local_cluster_embeddings_ = global_cluster_embeddings_[\n",
    "                np.array([j in lc for lc in local_clusters])\n",
    "            ]\n",
    "            indices = np.where(\n",
    "                (embeddings == local_cluster_embeddings_[:, None]).all(-1)\n",
    "            )[1]\n",
    "            for idx in indices:\n",
    "                all_local_clusters[idx] = np.append(\n",
    "                    all_local_clusters[idx], j + total_clusters\n",
    "                )\n",
    "\n",
    "        total_clusters += n_local_clusters\n",
    "\n",
    "    return all_local_clusters\n",
    "\n",
    "\n",
    "### --- Our code below --- ###\n",
    "\n",
    "\n",
    "def embed(texts):\n",
    "    \"\"\"\n",
    "    Generate embeddings for a list of text documents.\n",
    "\n",
    "    This function assumes the existence of an `embd` object with a method `embed_documents`\n",
    "    that takes a list of texts and returns their embeddings.\n",
    "\n",
    "    Parameters:\n",
    "    - texts: List[str], a list of text documents to be embedded.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: An array of embeddings for the given text documents.\n",
    "    \"\"\"\n",
    "    text_embeddings = embd.embed_documents(texts)\n",
    "    text_embeddings_np = np.array(text_embeddings)\n",
    "    return text_embeddings_np\n",
    "\n",
    "\n",
    "def embed_cluster_texts(texts):\n",
    "    \"\"\"\n",
    "    Embeds a list of texts and clusters them, returning a DataFrame with texts, their embeddings, and cluster labels.\n",
    "\n",
    "    This function combines embedding generation and clustering into a single step. It assumes the existence\n",
    "    of a previously defined `perform_clustering` function that performs clustering on the embeddings.\n",
    "\n",
    "    Parameters:\n",
    "    - texts: List[str], a list of text documents to be processed.\n",
    "\n",
    "    Returns:\n",
    "    - pandas.DataFrame: A DataFrame containing the original texts, their embeddings, and the assigned cluster labels.\n",
    "    \"\"\"\n",
    "    text_embeddings_np = embed(texts)  # Generate embeddings\n",
    "    cluster_labels = perform_clustering(\n",
    "        text_embeddings_np, 10, 0.1\n",
    "    )  # Perform clustering on the embeddings\n",
    "    df = pd.DataFrame()  # Initialize a DataFrame to store the results\n",
    "    df[\"text\"] = texts  # Store original texts\n",
    "    df[\"embd\"] = list(text_embeddings_np)  # Store embeddings as a list in the DataFrame\n",
    "    df[\"cluster\"] = cluster_labels  # Store cluster labels\n",
    "    return df\n",
    "\n",
    "\n",
    "def fmt_txt(df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Formats the text documents in a DataFrame into a single string.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the 'text' column with text documents to format.\n",
    "\n",
    "    Returns:\n",
    "    - A single string where all text documents are joined by a specific delimiter.\n",
    "    \"\"\"\n",
    "    unique_txt = df[\"text\"].tolist()\n",
    "    return \"--- --- \\n --- --- \".join(unique_txt)\n",
    "\n",
    "\n",
    "def embed_cluster_summarize_texts(\n",
    "    texts: List[str], level: int\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Embeds, clusters, and summarizes a list of texts. This function first generates embeddings for the texts,\n",
    "    clusters them based on similarity, expands the cluster assignments for easier processing, and then summarizes\n",
    "    the content within each cluster.\n",
    "\n",
    "    Parameters:\n",
    "    - texts: A list of text documents to be processed.\n",
    "    - level: An integer parameter that could define the depth or detail of processing.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple containing two DataFrames:\n",
    "      1. The first DataFrame (`df_clusters`) includes the original texts, their embeddings, and cluster assignments.\n",
    "      2. The second DataFrame (`df_summary`) contains summaries for each cluster, the specified level of detail,\n",
    "         and the cluster identifiers.\n",
    "    \"\"\"\n",
    "\n",
    "    # Embed and cluster the texts, resulting in a DataFrame with 'text', 'embd', and 'cluster' columns\n",
    "    df_clusters = embed_cluster_texts(texts)\n",
    "\n",
    "    # Prepare to expand the DataFrame for easier manipulation of clusters\n",
    "    expanded_list = []\n",
    "\n",
    "    # Expand DataFrame entries to document-cluster pairings for straightforward processing\n",
    "    for index, row in df_clusters.iterrows():\n",
    "        for cluster in row[\"cluster\"]:\n",
    "            expanded_list.append(\n",
    "                {\"text\": row[\"text\"], \"embd\": row[\"embd\"], \"cluster\": cluster}\n",
    "            )\n",
    "\n",
    "    # Create a new DataFrame from the expanded list\n",
    "    expanded_df = pd.DataFrame(expanded_list)\n",
    "\n",
    "    # Retrieve unique cluster identifiers for processing\n",
    "    all_clusters = expanded_df[\"cluster\"].unique()\n",
    "\n",
    "    print(f\"--Generated {len(all_clusters)} clusters--\")\n",
    "\n",
    "    # Summarization\n",
    "    template = \"\"\"Here is a sub-set of LangChain Expression Language doc. \n",
    "    \n",
    "    LangChain Expression Language provides a way to compose chain in LangChain.\n",
    "    \n",
    "    Give a detailed summary of the documentation provided.\n",
    "    \n",
    "    Documentation:\n",
    "    {context}\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    chain = prompt | model | StrOutputParser()\n",
    "\n",
    "    # Format text within each cluster for summarization\n",
    "    summaries = []\n",
    "    for i in all_clusters:\n",
    "        df_cluster = expanded_df[expanded_df[\"cluster\"] == i]\n",
    "        formatted_txt = fmt_txt(df_cluster)\n",
    "        summaries.append(chain.invoke({\"context\": formatted_txt}))\n",
    "\n",
    "    # Create a DataFrame to store summaries with their corresponding cluster and level\n",
    "    df_summary = pd.DataFrame(\n",
    "        {\n",
    "            \"summaries\": summaries,\n",
    "            \"level\": [level] * len(summaries),\n",
    "            \"cluster\": list(all_clusters),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return df_clusters, df_summary\n",
    "\n",
    "\n",
    "def recursive_embed_cluster_summarize(\n",
    "    texts: List[str], level: int = 1, n_levels: int = 3\n",
    ") -> Dict[int, Tuple[pd.DataFrame, pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Recursively embeds, clusters, and summarizes texts up to a specified level or until\n",
    "    the number of unique clusters becomes 1, storing the results at each level.\n",
    "\n",
    "    Parameters:\n",
    "    - texts: List[str], texts to be processed.\n",
    "    - level: int, current recursion level (starts at 1).\n",
    "    - n_levels: int, maximum depth of recursion.\n",
    "\n",
    "    Returns:\n",
    "    - Dict[int, Tuple[pd.DataFrame, pd.DataFrame]], a dictionary where keys are the recursion\n",
    "      levels and values are tuples containing the clusters DataFrame and summaries DataFrame at that level.\n",
    "    \"\"\"\n",
    "    results = {}  # Dictionary to store results at each level\n",
    "\n",
    "    # Perform embedding, clustering, and summarization for the current level\n",
    "    df_clusters, df_summary = embed_cluster_summarize_texts(texts, level)\n",
    "\n",
    "    # Store the results of the current level\n",
    "    results[level] = (df_clusters, df_summary)\n",
    "\n",
    "    # Determine if further recursion is possible and meaningful\n",
    "    unique_clusters = df_summary[\"cluster\"].nunique()\n",
    "    if level < n_levels and unique_clusters > 1:\n",
    "        # Use summaries as the input texts for the next level of recursion\n",
    "        new_texts = df_summary[\"summaries\"].tolist()\n",
    "        next_level_results = recursive_embed_cluster_summarize(\n",
    "            new_texts, level + 1, n_levels\n",
    "        )\n",
    "\n",
    "        # Merge the results from the next level into the current results dictionary\n",
    "        results.update(next_level_results)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1faa9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Generated 1 clusters--\n"
     ]
    }
   ],
   "source": [
    "# Build tree\n",
    "leaf_texts = docs_texts\n",
    "results = recursive_embed_cluster_summarize(leaf_texts, level=1, n_levels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a58df4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: (                                                text  \\\n",
       "  0  \\n\\n\\n\\n\\nLangChain Expression Language (LCEL)...   \n",
       "  1  \\n\\n\\n\\n\\nHow to use output parsers to parse a...   \n",
       "  2  \\n\\n\\n\\n\\nHow to do \"self-querying\" retrieval ...   \n",
       "  \n",
       "                                                  embd cluster  \n",
       "  0  [-0.0019035545410588384, 0.017021719366312027,...     [0]  \n",
       "  1  [-0.0006431303918361664, 0.03332868963479996, ...     [0]  \n",
       "  2  [-0.01113185752183199, 0.02663378044962883, -0...     [0]  ,\n",
       "                                             summaries  level  cluster\n",
       "  0  The provided documentation excerpts cover thre...      1        0)}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "189c4a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Initialize all_texts with leaf_texts\n",
    "all_texts = leaf_texts.copy()\n",
    "\n",
    "# Iterate through the results to extract summaries from each level and add them to all_texts\n",
    "for level in sorted(results.keys()):\n",
    "    # Extract summaries from the current level's DataFrame\n",
    "    summaries = results[level][1][\"summaries\"].tolist()\n",
    "    # Extend all_texts with the summaries from the current level\n",
    "    all_texts.extend(summaries)\n",
    "\n",
    "# Now, use all_texts to build the vectorstore with Chroma\n",
    "vectorstore = Chroma.from_texts(texts=all_texts, embedding=embd)\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "54648e17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x310d91e80>, search_kwargs={})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7fb74d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.13/site-packages/langsmith/client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'A RAG (Retrieval Augmented Generation) chain in LangChain is a sequence of components (Runnables) that retrieves information from a database or knowledge source and then uses that information to generate a response. Here\\'s a specific code example using LangChain to create a self-querying retriever, which is a type of RAG application:\\n\\n```python\\nfrom langchain.chains.query_constructor.schema import AttributeInfo\\nfrom langchain.retrievers.self_query.base import SelfQueryRetriever\\nfrom langchain_openai import ChatOpenAI\\n\\n# Define metadata fields and document content description\\nmetadata_field_info = [\\n    AttributeInfo(name=\"genre\", description=\"The genre of the movie.\", type=\"string\"),\\n    # Add other metadata fields here...\\n]\\ndocument_content_description = \"Brief summary of a movie\"\\n\\n# Instantiate a language model\\nllm = ChatOpenAI(temperature=0)\\n\\n# Create the self-querying retriever\\nretriever = SelfQueryRetriever.from_llm(\\n    llm,\\n    vectorstore,  # This should be a previously created vector store\\n    document_content_description,\\n    metadata_field_info,\\n)\\n\\n# Use the retriever to fetch documents based on a query\\nretriever.invoke(\"I want to watch a comedy movie from the 90s.\")\\n```\\n\\nIn this example, `SelfQueryRetriever` is used to create a retriever that can interpret natural language queries and retrieve relevant documents from a vector store. The `ChatOpenAI` model is used to construct queries that the retriever can understand. The `AttributeInfo` class is used to define metadata fields for the documents in the vector store.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Question\n",
    "rag_chain.invoke(\"How to define a RAG chain? Give me a specific code example.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
